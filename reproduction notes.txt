#make sure you have the following roles
-
-
-
-


our continer registry is gcr.io/qp-hcls-capability-2021-07

#Created a cheap pre-emptible VM instance on GCP with http and https allowed
	sudo apt-get update
	
	-- install docker
		sudo apt-get install docker-ce docker-ce-cli containerd.io
	-- if the above code does not work
		sudo apt-get install -y docker.io
	
	sudo chmod 666 /var/run/docker.sock
	gcloud auth configure-docker
	
	git clone https://github.com/deepmind/alphafold.git
	-- make changes to the run_alphafold.py file for separate MSA and Structure
	-- make changes in the docker file and commend out the last 2 commands
	-- make changes in the dockerfile, comment out the pip install upgrade jax and jaxlib command and replace it with 
		pip install "jax[cuda11_cudnn805]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

	-- I have already made the above changes and pushed to a git repo. 
		https://github.com/Tehemton/alphafold_test.git
		will push to our gitlab quantiphi private repos once I have the access

	

	cd alphafold/
		docker build -f docker/Dockerfile -t alphafold .
		-- to push to docker hub
			docker tag alphafold tehemton/alphafold-batch:alphafold_2.2.2_separated
			docker login docker.io
			docker push tehemton/alphafold-batch:alphafold_2.2.2_separated
		
		-- use cloud storage and compute instance images to create a custom image (could not get it to work)
			docker save --output alphafold.tar.gz alphafold:latest
			gsutil cp alphafold.tar.gz gs://alphafold-bucket-1/
			-- now in the compute instance image section, create an image using this .tar.gz file

		-- to push to gcp container registry. No way to create compute engine images from here as well.
			-- enable container registry
			docker tag alphafold us.gcr.io/vertex-test-1/alphafold_t1:2.2.2_split_processes
			gcloud auth configure-docker
			docker push us.gcr.io/vertex-test-1/alphafold_t1:2.2.2_split_processes

		-- do not use the bloody artifact registry. No way to create custom image from artifact registry docker images. Plus acceess control is unnecessarily messy
		-- to push to gcp artifact registry
			-- initialize artifact registry
			-- create repository
			-- grant the compute engine service account admin/writer access to this repo (have not been able to do this yet, I granted allUser access)
			-- stop the compute engine instance and run the below code (change instance name and zone)
				gcloud compute instances set-service-account instance-1 --scopes=storage-rw --zone=us-central1-a
			-- restart the compute engine instance 
			
			gcloud auth configure-docker 
			docker tag alphafold us-central1-docker.pkg.dev/vertex-test-1/alphafold-container-registry/alphafold_2.2.2_split_processes
			docker push us-central1-docker.pkg.dev/vertex-test-1/alphafold-container-registry/alphafold_2.2.2_split_processes
			
			docker tag alphafold us.gcr.io/qp-hcls-capability-2021-07/alphafold:2.2.2_separated
			docker push us.gcr.io/qp-hcls-capability-2021-07/alphafold:2.2.2_separated


	
# create new instance with 2.6tb ssd pd and custom image tehemton/alphafold-batch:alphafold_2.2.2_separated
# attach a 2.6tb ssd
-- ssh into the new VM
-- wait for the container to start
--check container status using 
	docker container ps
-- format the new drive as per instructions below and mount
-- go into the container using
	docker exec -it [CONTAINER_NAME] bash
	or docker attach [CONTAINER_NAME]

# alternatively you can also manualy docker pull the image from tehemton/alphafold-batch:alphafold_2.2.2_separated
# attach a 2.2tb ssd
# ssh into the new VM
docker pull tehemton/alphafold-batch:alphafold_2.2.2_separated
-- format the new 2.6tb drive as described below and mount 
	-- https://cloud.google.com/compute/docs/disks/add-persistent-disk?&_ga=2.137279984.-1390412617.1647495718#formatting
	-- the disk is mounted as alphafold_storage/
	-- in the root diretory, make the following folders inside alphafold_storage
		fasta_files		prot_db		results

docker run -it -v /alphafold_storage/:/alphafold_storage tehemton/alphafold-batch:alphafold_2.2.2_separated bash
	-- install download dependencies
		apt-get update
		apt-get install rsync -y
		apt-get install aria2 -y
		apt-get install tmux
		apt-get install nano

	-- run the database download
		app/alphafold/scripts/download_all_data.sh alphafold_storage/prot_db/

	-- the following database download scripts throw a gunzip error. 
		It is better to download these through the script and decompress manually using gz -d [FILENAME]
		mgnify
		small_bfd
		uniprot
		uniref90 


############################################################
now to run predictions
############################################################

Create 2 VM instances
	-- alphafold-feature-search = 16 CPUs and 0 GPU (e2-standard-16)
	-- alphafold-structure-prediction = 04 CPU and A100 GPU (a2-highgpu-1g)

-- ssh into both and install dependencies for one time setup
	sudo apt-get update
	sudo apt-get install tmux -y
	sudo apt-get install nano -y
	sudo apt-get install runc -y
	-- cd to home
	sudo mkdir /alphafold_storage/
	## best way to install latest docker https://docs.docker.com/engine/install/debian/
	## use the above process instead of this one sudo apt-get install -y docker.io
	systemctl start docker
	sudo chmod 666 /var/run/docker.sock
	docker pull tehemton/alphafold-batch:alphafold_2.2.2_separated


--now to run either the features or structure prediction part, 
	-- make sure the pd is not being used by another VM.
	-- attach the persistent disk as shown below. Through cloud shell. can be done through VM is the service account has the correct permissions
	gcloud compute instances attach-disk alphafold-feature-search-16 --disk alphafold-full-protein-db --zone=us-central1-a
	or
	gcloud compute instances attach-disk alphafold-structure-prediction --disk alphafold-full-protein-db --zone=us-central1-a

	-- once attached, start the VM and ssh into it
	-- run the below command to get the disk name
	sudo lsblk
	-- run the below command to mount the disk
	sudo mount -o discard,defaults /dev/sdb /alphafold_storage/
	sudo chmod a+w /alphafold_storage

	-- now you can simply run the docker container, attach the disk as a volume. Make sure to tmux first
	tmux new -s prot
	--to reconnect to a tmux runtime run
	tmux a -t prot

	-- upload the fsta files to the correct location

	docker run -it -v /alphafold_storage/:/alphafold_storage tehemton/alphafold-batch:alphafold_2.2.2_separated bash
	or
	docker run -it --gpus all -v /alphafold_storage/:/alphafold_storage tehemton/alphafold-batch:alphafold_2.2.2_separated bash

	-- to log anything to a file, use 
	SOME_COMMAND 2>&1 | tee alphafold_storage/results/debug.txt

	-- once inside the docker container, run the pipeline

	-- on completion of the process, you can download the results and exit the container
	-- then before exiting the VM, always unmount and detach the disk
	sudo umount /dev/sdb
	gcloud compute instances detach-disk alphafold-feature-search-16 --disk=alphafold-full-protein-db --zone=us-central1-a
	or
	gcloud compute instances detach-disk alphafold-structure-prediction --disk alphafold-full-protein-db --zone=us-central1-a


python app/alphafold/run_alphafold.py \
--fasta_paths=/alphafold_storage/fasta_files/1A2K.fasta \
--output_dir=/alphafold_storage/results/ \
--data_dir=/alphafold_storage/prot_db/ \
--uniref90_database_path=/alphafold_storage/prot_db/uniref90/uniref90.fasta \
--mgnify_database_path=/alphafold_storage/prot_db/mgnify/mgy_clusters_2018_12.fa \
--bfd_database_path=/alphafold_storage/prot_db/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \
--uniclust30_database_path=/alphafold_storage/prot_db/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \
--uniprot_database_path=/alphafold_storage/prot_db/uniprot/uniprot.fasta \
--pdb_seqres_database_path=/alphafold_storage/prot_db/pdb_seqres/pdb_seqres.txt \
--template_mmcif_dir=/alphafold_storage/prot_db/pdb_mmcif/mmcif_files/ \
--obsolete_pdbs_path=/alphafold_storage/prot_db/pdb_mmcif/obsolete.dat \
--max_template_date=2000-01-01 \
--use_gpu_relax=True \
--model_preset=multimer \
--process_type=full

python app/alphafold/run_alphafold_og.py \
--fasta_paths=/alphafold_storage/fasta_files/1A2K.fasta \
--output_dir=/alphafold_storage/results/ \
--data_dir=/alphafold_storage/prot_db/ \
--uniref90_database_path=/alphafold_storage/prot_db/uniref90/uniref90.fasta \
--mgnify_database_path=/alphafold_storage/prot_db/mgnify/mgy_clusters_2018_12.fa \
--bfd_database_path=/alphafold_storage/prot_db/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \
--uniclust30_database_path=/alphafold_storage/prot_db/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \
--uniprot_database_path=/alphafold_storage/prot_db/uniprot/uniprot.fasta \
--pdb_seqres_database_path=/alphafold_storage/prot_db/pdb_seqres/pdb_seqres.txt \
--template_mmcif_dir=/alphafold_storage/prot_db/pdb_mmcif/mmcif_files/ \
--obsolete_pdbs_path=/alphafold_storage/prot_db/pdb_mmcif/obsolete.dat \
--max_template_date=2000-01-01 \
--use_gpu_relax=True \
--model_preset=multimer

python app/alphafold/run_alphafold.py \
--fasta_paths=/alphafold_storage/fasta_files/setup_test.fasta \
--data_dir=/alphafold_storage/prot_db/ \
--output_dir=/alphafold_storage/results/ \
--uniref90_database_path=/alphafold_storage/prot_db/uniref90/uniref90.fasta \
--mgnify_database_path=/alphafold_storage/prot_db/mgnify/mgy_clusters_2018_12.fa \
--bfd_database_path=/alphafold_storage/prot_db/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \
--uniclust30_database_path=/alphafold_storage/prot_db/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \
--pdb70_database_path=/alphafold_storage/prot_db/pdb70/pdb70 \
--template_mmcif_dir=/alphafold_storage/prot_db/pdb_mmcif/mmcif_files/ \
--obsolete_pdbs_path=/alphafold_storage/prot_db/pdb_mmcif/obsolete.dat \
--max_template_date=2000-01-01 \
--use_gpu_relax=True \
--model_preset=monomer \
--process_type=full


the original alphafold throws AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft' error 
my alphafold image throws some vague error on multimers: simtk.openmm.OpenMMException: Particle coordinate is nan
